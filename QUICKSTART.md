# å¿«é€Ÿå¼€å§‹æŒ‡å—

## ğŸš€ 5åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# å¯åŠ¨ Redis (é€‰æ‹©ä¸€ä¸ªæ–¹å¼)
# macOS
brew services start redis

# Ubuntu
sudo systemctl start redis

# Docker
docker run -p 6379:6379 redis:latest
```

### 2. åˆ›å»ºæµ‹è¯•æ•°æ®

```python
from data.redis_connector import RedisConnector

# è¿æ¥ Redis
connector = RedisConnector()

# å­˜å‚¨æµ‹è¯•æ•°æ®
connector.store_json_variable("test", "user_info", {
    "name": "å¼ ä¸‰",
    "age": 30,
    "city": "åŒ—äº¬"
})

connector.store_direct_variable("test", "store_count", "100")
```

### 3. åŸºæœ¬ä½¿ç”¨

```python
from data.redis_connector import RedisConnector
from data.dataloader import DataLoader
from core.engine import RuleEngine
from core.nodes import LogicNode

# 1. åˆå§‹åŒ–
connector = RedisConnector()
loader = DataLoader(connector)
engine = RuleEngine("demo")

# 2. å®šä¹‰æ•°æ®é…ç½®
data_config = {
    "namespace": "test",
    "placeholder": [
        {"name": "store_nbr", "description": "é—¨åº—å·"}
    ],
    "variable": [
        {
            "name": "user_info",
            "redis_config": {
                "prefix": [],
                "field": "user_info",
                "type": "json"
            }
        },
        {
            "name": "store_count",
            "redis_config": {
                "prefix": [],
                "field": "store_count",
                "type": "value"
            }
        }
    ]
}

# 3. åŠ è½½æ•°æ®
placeholders = {"store_nbr": "123"}
loaded_data = loader.load_data(data_config, placeholders, "2025-01-01 00:00:00")

# 4. åˆ›å»ºè§„åˆ™
logic_node = LogicNode("process")
logic_node.set_logic("""
# å¤„ç†æ•°æ®
user_name = user_info.get('name', 'æœªçŸ¥')
user_age = user_info.get('age', 0)
total_stores = int(store_count)

# è®¡ç®—ç»“æœ
result = f"{user_name}ç®¡ç†{total_stores}å®¶é—¨åº—ï¼Œå¹´é¾„{user_age}å²"
""")
logic_node.set_tracked_variables(["result"])

# 5. æ‰§è¡Œå¼•æ“
engine.add_dependency(None, logic_node)
results = engine.execute("2025-01-01 00:00:00", placeholders, loaded_data)

# 6. æŸ¥çœ‹ç»“æœ
print("æ‰§è¡Œç»“æœ:")
for node_name, result in results.items():
    if result.success:
        print(f"âœ… {node_name}: {result.data}")
    else:
        print(f"âŒ {node_name}: {result.error}")
```

## ğŸ“Š æ•°æ®ç±»å‹ç¤ºä¾‹

### Value ç±»å‹
```python
# å­˜å‚¨
connector.store_direct_variable("test", "count", "100")

# é…ç½®
{
    "name": "count",
    "redis_config": {
        "field": "count",
        "type": "value"
    }
}
```

### JSON ç±»å‹
```python
# å­˜å‚¨
connector.store_json_variable("test", "config", {
    "enabled": True,
    "threshold": 0.8
})

# é…ç½®
{
    "name": "config",
    "redis_config": {
        "field": "config",
        "type": "json"
    }
}
```

### Timeseries ç±»å‹
```python
# å­˜å‚¨
import time
timestamp = time.time()
connector.add_timeseries_point("test", "sales", timestamp, {
    "amount": 1000,
    "quantity": 10
})

# é…ç½®
{
    "name": "sales_data",
    "redis_config": {
        "field": "sales",
        "type": "timeseries",
        "from_datetime": "${yyyy-MM-dd-7d}",
        "to_datetime": "${yyyy-MM-dd}",
        "drop_duplicate": "keep_latest"
    }
}
```

### Densets ç±»å‹
```python
# å­˜å‚¨ - ä½¿ç”¨schemaä¿è¯å­—æ®µé¡ºåº
schema = "date:string,qty:int,price:double"
connector.add_densets_point("test", "forecast", timestamp, {
    "price": 10.5,  # å­—æ®µé¡ºåºä¸é‡è¦ï¼Œschemaä¼šä¿è¯é¡ºåº
    "date": "2025-01-01",
    "qty": 100
}, schema=schema)

# é…ç½®
{
    "name": "forecast_data",
    "redis_config": {
        "field": "forecast",
        "type": "densets",
        "split": ",",
        "schema": "date:string,qty:int,price:double"
    }
}
```

## ğŸ”§ èŠ‚ç‚¹ç±»å‹ç¤ºä¾‹

### LogicNode - é€»è¾‘å¤„ç†
```python
logic_node = LogicNode("calculate")
logic_node.set_logic("""
# è®¡ç®—å¹³å‡å€¼
avg_value = sum(values) / len(values)
result = {"average": avg_value, "count": len(values)}
""")
logic_node.set_tracked_variables(["result"])
```

### GateNode - æ¡ä»¶åˆ¤æ–­
```python
gate_node = GateNode("check_threshold")
gate_node.set_condition("value > 100")
```

### CollectionNode - æ•°æ®æ”¶é›†
```python
collection_node = CollectionNode("merge_data")
collection_node.set_logic("""
import pandas as pd
# åˆå¹¶æ‰€æœ‰ä¸Šæ¸¸æ•°æ®
df_list = []
for node_name in collection:
    if 'data' in collection[node_name]:
        df_list.append(collection[node_name]['data'])
        
if df_list:
    output = pd.concat(df_list, ignore_index=True)
else:
    output = pd.DataFrame()
""")
collection_node.set_tracked_variables(["output"])
```

## ğŸ“‹ Schema éªŒè¯ç¤ºä¾‹

### èŠ‚ç‚¹è¾“å…¥éªŒè¯
```python
logic_node.add_expected_input_schema("user_count", "int")
logic_node.add_expected_input_schema("sales_data", "ds:string,amount:double,quantity:int")
```

## ğŸš€ æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. æ‰¹é‡æ•°æ®åŠ è½½
```python
# ä½¿ç”¨æ‰¹é‡åŠ è½½æé«˜æ€§èƒ½
loaded_data = loader.load_data_batch(data_config, placeholders, job_datetime)
```

### 2. åˆç†è®¾ç½® TTL
```python
# è®¾ç½®æ•°æ®è¿‡æœŸæ—¶é—´
connector.store_json_variable("test", "data", value, ttl=3600)  # 1å°æ—¶è¿‡æœŸ
```

### 3. ä½¿ç”¨è¿æ¥æ± 
```python
# Redis è¿æ¥å™¨è‡ªåŠ¨ä½¿ç”¨è¿æ¥æ± 
connector = RedisConnector(
    host='localhost',
    port=6379,
    db=0
)
```

## ğŸ§ª è¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
python run_tests.py

# è¿è¡Œç‰¹å®šæµ‹è¯•
cd test && python test_densets.py

# è¿è¡Œç¤ºä¾‹
python example_engine.py
python example_forecast.py
```

## ğŸ“ è·å–å¸®åŠ©

- ğŸ“– å®Œæ•´æ–‡æ¡£: æŸ¥çœ‹ `README.md`
- ğŸ§ª æµ‹è¯•ç¤ºä¾‹: æŸ¥çœ‹ `test/` ç›®å½•
- ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹: æŸ¥çœ‹ `example_*.py` æ–‡ä»¶
- ğŸ› é—®é¢˜åé¦ˆ: æäº¤ Issue

## ğŸ¯ ä¸‹ä¸€æ­¥

1. é˜…è¯»å®Œæ•´çš„ `README.md` äº†è§£æ‰€æœ‰åŠŸèƒ½
2. æŸ¥çœ‹ `example_forecast.py` äº†è§£å¤æ‚åœºæ™¯çš„ä½¿ç”¨
3. è¿è¡Œæµ‹è¯•ç¡®ä¿ç¯å¢ƒæ­£å¸¸
4. å¼€å§‹æ„å»ºä½ çš„è§„åˆ™å¼•æ“åº”ç”¨ï¼ 