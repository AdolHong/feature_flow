#!/usr/bin/env python3
"""
ç¤ºä¾‹ç¨‹åºï¼šä»RedisåŠ è½½æ•°æ®å¹¶ä½¿ç”¨è§„åˆ™å¼•æ“å¤„ç†
"""
import json
import pandas as pd

from data.redis_connector import RedisConnector
from data.dataloader import DataLoader
from core.engine import RuleEngine
from core.nodes import LogicNode, GateNode, CollectionNode


def create_data_config():
    """åˆ›å»ºæ•°æ®é…ç½®"""
    return {
        "namespace": "sams_cloud_demand_fcst_tn",
        "placeholder": [
            {"name": "store_nbr", "alias": None, "description": "é—¨åº—å·"},
            {"name": "item_nbr", "alias": None, "description": "å•†å“å·"},
        ],
        "variable": [
            # å•†å“ç»´åº¦ä¿¡æ¯
            {
                "name": "dim_item",
                "alias": None,
                "description": "å•†å“ç»´åº¦ä¿¡æ¯",
                "namespace": "sams",
                "redis_config": {
                    "prefix": [
                        {"key": "item_nbr", "value": "${item_nbr}"}
                    ],
                    "field": "dim_item",
                    "type": "json"
                }
            },
            # é—¨åº—ç»´åº¦ä¿¡æ¯
            {
                "name": "dim_store",
                "alias": None,
                "description": "é—¨åº—ç»´åº¦ä¿¡æ¯",
                "namespace": "sams_cloud",
                "redis_config": {
                    "prefix": [
                        {"key": "store_nbr", "value": "${store_nbr}"}
                    ],
                    "field": "dim_store",
                    "type": "json"
                }
            },
            # XGBé¢„æµ‹æ•°æ®
            {
                "name": "xgb_forecast",
                "alias": None,
                "description": "XGBé¢„æµ‹æ•°æ®",
                "namespace": None,
                "redis_config": {
                    "prefix": [
                        {"key": "store_nbr", "value": "${store_nbr}"},
                        {"key": "item_nbr", "value": "${item_nbr}"}
                    ],
                    "field": "xgb_restore__lgb_0_2024-08-13",
                    "type": "timeseries",
                    "from_datetime": "${yyyy-MM-dd}",
                    "to_datetime": "${yyyy-MM-dd+6d}",
                }
            },
            # TFTé¢„æµ‹æ•°æ®
            {
                "name": "tft_forecast",
                "alias": None,
                "description": "TFTé¢„æµ‹æ•°æ®",
                "namespace": None,
                "redis_config": {
                    "prefix": [
                        {"key": "store_nbr", "value": "${store_nbr}"},
                        {"key": "item_nbr", "value": "${item_nbr}"},
                    ],
                    "field": "tft_restore_20240926",
                    "type": "timeseries",
                    "from_datetime": "${yyyy-MM-dd}",
                    "to_datetime": "${yyyy-MM-dd+6d}",
                }
            }
        ]
    }


def create_rule_engine():
    """åˆ›å»ºè§„åˆ™å¼•æ“"""
    engine = RuleEngine("sams_demand_forecast_engine")
    
    # åˆ›å»ºé€»è¾‘èŠ‚ç‚¹1ï¼šæ•°æ®é¢„å¤„ç†
    preprocess_node = LogicNode("preprocess")
    preprocess_node.set_logic("""
# æå–åŸå¸‚å’Œåˆ†ç±»
city_cn = dim_store.get('city_cn', 'æœªçŸ¥åŸå¸‚')
category_nbr = int(dim_item.get('category_nbr', 0))
is_fresh = dim_item.get('is_fresh', 0)
""")
    preprocess_node.set_tracked_variables(["city_cn", "category_nbr", "is_fresh"])
    
    gate_cate66_node = GateNode("gate_cate66")
    gate_cate66_node.set_condition("category_nbr == 66")
    
    # åˆ›å»ºé€»è¾‘èŠ‚ç‚¹2ï¼šé¢„æµ‹åˆ†æ
    tft_forecast_node = LogicNode("tft_forecast")
    tft_forecast_node.set_logic("""
tft_forecast['priority'] = 90
tft_forecast['model'] = 'tft'
output = tft_forecast
""")
    tft_forecast_node.set_tracked_variables(["output"])

    base_forecast_node = LogicNode("base_forecast")
    base_forecast_node.set_logic("""
xgb_forecast['priority'] = 80
xgb_forecast['model'] = 'xgb'
output = xgb_forecast
""")
    base_forecast_node.set_tracked_variables(["output"])

    coll_node = CollectionNode("coll")
    coll_node.add_expected_input_schema("output", "job_date:string,fcst_date:string,fcst_qty:double,etl_time:string,priority:int,model:string")
    coll_node.set_logic(
"""
import pandas as pd     
# æ‹¼æ¥ä¸Šæ¸¸çš„è¾“å‡º; ä¸å…³å¿ƒnode_name      
df_concat = pd.concat([collection[node_name]['output'] for node_name in collection])
df_concat.sort_values(by=["fcst_date", "model"], inplace=True) # æŒ‰fcst_dateå’Œmodelæ’åº

# æŒ‰fcst_dateåˆ†ç»„ï¼Œå–priorityæœ€é«˜çš„è¡Œï¼Œå¹¶è®¡ç®—å¹³å‡å€¼
output = df_concat.groupby("fcst_date").apply(
    lambda group: group[group["priority"] == group["priority"].max()].mean(numeric_only=True)
).reset_index()
                        
# è¡¥å……éæ•°å€¼åˆ—ï¼Œå¹¶å°†modelæ‹¼æ¥ä¸ºå­—ç¬¦ä¸²
for fcst_date in output["fcst_date"]:
    original_group = df_concat[df_concat['fcst_date'] == fcst_date]
    max_priority_group = original_group[original_group["priority"] == original_group["priority"].max()]
    
    # è·å–æœ€é«˜priorityçš„æ‰€æœ‰modelï¼Œç”¨é€—å·æ‹¼æ¥
    models = max_priority_group["model"].tolist()
    model_str = ",".join(models)
    
    # å–ç¬¬ä¸€è¡Œçš„å…¶ä»–éæ•°å€¼åˆ—
    first_row = max_priority_group.iloc[0]
    output.loc[output["fcst_date"] == fcst_date, "model"] = model_str

output['job_date'] = "${yyyy-MM-dd}"
output['store_nbr'] = store_nbr
output['item_nbr'] = item_nbr
""")
    coll_node.set_tracked_variables(["output", "df_concat"])

    
    # è®¾ç½®ä¾èµ–å…³ç³»
    engine.add_dependency(None, preprocess_node)
    engine.add_dependency(preprocess_node, base_forecast_node)
    engine.add_dependency(preprocess_node, gate_cate66_node)
    engine.add_dependency(gate_cate66_node, tft_forecast_node)
    engine.add_dependency(base_forecast_node, coll_node)
    engine.add_dependency(tft_forecast_node, coll_node)
    
    return engine


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ‰§è¡Œå±±å§†äº‘ä»“éœ€æ±‚é¢„æµ‹åˆ†æç¤ºä¾‹")
    
    # å‚æ•°è®¾ç½®
    store_nbr = "6111"
    item_nbr = "981049286"
    job_datetime = "2025-07-22 01:00:00"
    
    # è·å–å½“å‰æ—¥æœŸå’Œæ—¶é—´
    # import datetime
    # current_time = datetime.datetime.now()
    # formatted_time = current_time.strftime("%Y-%m-%d %H:%M:%S") # æ ¼å¼åŒ–ä¸º "YYYY-MM-DD HH:MM:SS" æ ¼å¼çš„å­—ç¬¦ä¸²
    # print(formatted_time)


    
    print(f"ğŸ“Š åˆ†æå‚æ•°:")
    print(f"  é—¨åº—å·: {store_nbr}")
    print(f"  å•†å“å·: {item_nbr}")
    print(f"  ä½œä¸šæ—¶é—´: {job_datetime}")
    
    try:
        # 1. åˆå§‹åŒ–Redisè¿æ¥å™¨
        print("\nğŸ”Œ åˆå§‹åŒ–Redisè¿æ¥...")
        connector = RedisConnector()
        
        # 2. åˆ›å»ºæ•°æ®åŠ è½½å™¨
        print("ğŸ“¥ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        loader = DataLoader(connector)
        
        # 3. åˆ›å»ºæ•°æ®é…ç½®
        print("âš™ï¸ åˆ›å»ºæ•°æ®é…ç½®...")
        data_config = create_data_config()
        
        # 4. åŠ è½½æ•°æ®
        print("ğŸ“Š ä»RedisåŠ è½½æ•°æ®...")
        placeholders = {
            "store_nbr": store_nbr,
            "item_nbr": item_nbr
        }
        
        loaded_data = loader.load_data(data_config, placeholders, job_datetime)
        
        print("âœ… æ•°æ®åŠ è½½å®Œæˆ:")
        for var_name, var_data in loaded_data.items():
            if isinstance(var_data, pd.DataFrame):
                print(f"  {var_name}: DataFrame({len(var_data)}è¡Œ)")
            else:
                print(f"  {var_name}: {type(var_data).__name__}")
        
        # 5. åˆ›å»ºè§„åˆ™å¼•æ“
        print("\nâš™ï¸ åˆ›å»ºè§„åˆ™å¼•æ“...")
        engine = create_rule_engine()
        
        # 6. æ‰§è¡Œè§„åˆ™å¼•æ“
        print("ğŸ”„ æ‰§è¡Œè§„åˆ™å¼•æ“...")
        results = engine.execute(job_datetime, placeholders=placeholders, variables=loaded_data)
        
        # 8. è¾“å‡ºç»“æœ
        print("\nğŸ“‹ æ‰§è¡Œç»“æœ:")
        for node_name, result in results.items():
            if result.success:
                print(f"  âœ… {node_name}: æ‰§è¡ŒæˆåŠŸ")
            else:
                print(f"  âŒ {node_name}: æ‰§è¡Œå¤±è´¥ - {result.error}")
        
        # 9. è·å–æœ€ç»ˆç»“æœ
        print("\nğŸ¯ æœ€ç»ˆåˆ†æç»“æœ:")
        final_context = engine.get_node_flow_context("result_summary")
        if "final_report" in final_context:
            final_report = final_context["final_report"][0]
            print(json.dumps(final_report, ensure_ascii=False, indent=2))
        
        # 10. è¾“å‡ºæ‰§è¡Œæ‘˜è¦
        print("\nğŸ“Š æ‰§è¡Œæ‘˜è¦:")
        summary = engine.get_execution_summary()
        print(f"  æ€»èŠ‚ç‚¹æ•°: {summary['total_nodes']}")
        print(f"  æˆåŠŸèŠ‚ç‚¹: {summary['successful_nodes']}")
        print(f"  å¤±è´¥èŠ‚ç‚¹: {summary['failed_nodes']}")
        print(f"  é˜»å¡èŠ‚ç‚¹: {summary['blocked_nodes']}")
        print(f"  æˆåŠŸç‡: {summary['success_rate']:.2%}")
        
        print("\nâœ… ç¤ºä¾‹ç¨‹åºæ‰§è¡Œå®Œæˆ!")
        
    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 