#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ€§èƒ½æµ‹è¯•è„šæœ¬ï¼šå¯¹æ¯”å•æ¬¡æŸ¥è¯¢å’Œæ‰¹é‡æŸ¥è¯¢çš„æ€§èƒ½å·®å¼‚
"""

import time
import json
from data.redis_connector import RedisConnector
from data.dataloader import DataLoader
from typing import Dict


def create_test_data(redis_connector: RedisConnector, namespace: str, count: int = 1000):
    """
    åˆ›å»ºæµ‹è¯•æ•°æ®
    
    Args:
        redis_connector: Redisè¿æ¥å™¨
        namespace: å‘½åç©ºé—´
        count: æ•°æ®æ•°é‡
    """
    print(f"ğŸ”„ æ­£åœ¨åˆ›å»º {count} æ¡æµ‹è¯•æ•°æ®...")
    
    # åˆ›å»ºvalueç±»å‹æ•°æ®
    for i in range(count):
        key = f"test_value_{i}"
        value = f"value_{i}"
        redis_connector.store_direct_variable(namespace, key, value)
    
    # åˆ›å»ºjsonç±»å‹æ•°æ®
    for i in range(count):
        key = f"test_json_{i}"
        value = {
            "id": i,
            "name": f"item_{i}",
            "data": {
                "score": i * 10,
                "status": "active" if i % 2 == 0 else "inactive"
            }
        }
        redis_connector.store_json_variable(namespace, key, value)
    
    print(f"âœ… æµ‹è¯•æ•°æ®åˆ›å»ºå®Œæˆï¼Œå…± {count * 2} æ¡è®°å½•")


def create_test_config(count: int = 1000):
    """
    åˆ›å»ºæµ‹è¯•é…ç½®
    
    Args:
        count: å˜é‡æ•°é‡
        
    Returns:
        æµ‹è¯•é…ç½®å­—å…¸
    """
    variables = []
    
    # æ·»åŠ valueç±»å‹å˜é‡
    for i in range(count):
        variables.append({
            "name": f"value_var_{i}",
            "redis_config": {
                "type": "value",
                "field": f"test_value_{i}",
                "prefix": [
                    {"key": "env", "value": "prod"},
                    {"key": "region", "value": "cn"}
                ]
            }
        })
    
    # æ·»åŠ jsonç±»å‹å˜é‡
    for i in range(count):
        variables.append({
            "name": f"json_var_{i}",
            "redis_config": {
                "type": "json",
                "field": f"test_json_{i}",
                "prefix": [
                    {"key": "env", "value": "prod"},
                    {"key": "region", "value": "cn"}
                ]
            }
        })
    
    return {
        "namespace": "test_performance",
        "placeholder": [
            {"name": "env", "value": "prod"},
            {"name": "region", "value": "cn"}
        ],
        "variable": variables
    }


def test_single_query(data_loader: DataLoader, data_config: Dict, placeholders: Dict, job_datetime: str, iterations: int = 10):
    """
    æµ‹è¯•å•æ¬¡æŸ¥è¯¢æ€§èƒ½
    
    Args:
        data_loader: æ•°æ®åŠ è½½å™¨
        data_config: æ•°æ®é…ç½®
        placeholders: å ä½ç¬¦
        job_datetime: ä½œä¸šæ—¶é—´
        iterations: æµ‹è¯•è¿­ä»£æ¬¡æ•°
        
    Returns:
        å¹³å‡è€—æ—¶ï¼ˆç§’ï¼‰
    """
    print(f"ğŸ”„ å¼€å§‹å•æ¬¡æŸ¥è¯¢æ€§èƒ½æµ‹è¯•ï¼Œ{iterations} æ¬¡è¿­ä»£...")
    
    times = []
    for i in range(iterations):
        start_time = time.time()
        try:
            result = data_loader.load_data(data_config, placeholders, job_datetime)
            end_time = time.time()
            elapsed = end_time - start_time
            times.append(elapsed)
            print(f"  ç¬¬ {i+1} æ¬¡: {elapsed:.4f} ç§’")
        except Exception as e:
            print(f"  ç¬¬ {i+1} æ¬¡å¤±è´¥: {e}")
    
    if times:
        avg_time = sum(times) / len(times)
        print(f"âœ… å•æ¬¡æŸ¥è¯¢å¹³å‡è€—æ—¶: {avg_time:.4f} ç§’")
        return avg_time
    else:
        print("âŒ å•æ¬¡æŸ¥è¯¢æµ‹è¯•å¤±è´¥")
        return None


def test_batch_query(data_loader: DataLoader, data_config: Dict, placeholders: Dict, job_datetime: str, iterations: int = 10):
    """
    æµ‹è¯•æ‰¹é‡æŸ¥è¯¢æ€§èƒ½
    
    Args:
        data_loader: æ•°æ®åŠ è½½å™¨
        data_config: æ•°æ®é…ç½®
        placeholders: å ä½ç¬¦
        job_datetime: ä½œä¸šæ—¶é—´
        iterations: æµ‹è¯•è¿­ä»£æ¬¡æ•°
        
    Returns:
        å¹³å‡è€—æ—¶ï¼ˆç§’ï¼‰
    """
    print(f"ğŸ”„ å¼€å§‹æ‰¹é‡æŸ¥è¯¢æ€§èƒ½æµ‹è¯•ï¼Œ{iterations} æ¬¡è¿­ä»£...")
    
    times = []
    for i in range(iterations):
        start_time = time.time()
        try:
            result = data_loader.load_data_batch(data_config, placeholders, job_datetime)
            end_time = time.time()
            elapsed = end_time - start_time
            times.append(elapsed)
            print(f"  ç¬¬ {i+1} æ¬¡: {elapsed:.4f} ç§’")
        except Exception as e:
            print(f"  ç¬¬ {i+1} æ¬¡å¤±è´¥: {e}")
    
    if times:
        avg_time = sum(times) / len(times)
        print(f"âœ… æ‰¹é‡æŸ¥è¯¢å¹³å‡è€—æ—¶: {avg_time:.4f} ç§’")
        return avg_time
    else:
        print("âŒ æ‰¹é‡æŸ¥è¯¢æµ‹è¯•å¤±è´¥")
        return None


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Redisæ‰¹é‡æŸ¥è¯¢æ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    
    # è¿æ¥Redis
    try:
        redis_connector = RedisConnector()
        data_loader = DataLoader(redis_connector)
        print("âœ… Redisè¿æ¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ Redisè¿æ¥å¤±è´¥: {e}")
        return
    
    # æµ‹è¯•å‚æ•°
    test_counts = [10, 50, 100, 500, 1000]  # æµ‹è¯•ä¸åŒæ•°é‡çš„å˜é‡
    iterations = 5  # æ¯ä¸ªæµ‹è¯•çš„è¿­ä»£æ¬¡æ•°
    
    # æµ‹è¯•é…ç½®
    placeholders = {"env": "prod", "region": "cn"}
    job_datetime = "2024-01-01 12:00:00"
    
    results = []
    
    for count in test_counts:
        print(f"\nğŸ“Š æµ‹è¯• {count} ä¸ªå˜é‡")
        print("-" * 30)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        create_test_data(redis_connector, "test_performance", count)
        
        # åˆ›å»ºæµ‹è¯•é…ç½®
        data_config = create_test_config(count)
        
        # æµ‹è¯•å•æ¬¡æŸ¥è¯¢
        single_time = test_single_query(data_loader, data_config, placeholders, job_datetime, iterations)
        
        # æµ‹è¯•æ‰¹é‡æŸ¥è¯¢
        batch_time = test_batch_query(data_loader, data_config, placeholders, job_datetime, iterations)
        
        # è®¡ç®—æ€§èƒ½æå‡
        if single_time and batch_time:
            improvement = (single_time - batch_time) / single_time * 100
            speedup = single_time / batch_time
            results.append({
                "count": count,
                "single_time": single_time,
                "batch_time": batch_time,
                "improvement": improvement,
                "speedup": speedup
            })
            
            print(f"ğŸ“ˆ æ€§èƒ½æå‡: {improvement:.2f}%")
            print(f"ğŸš€ é€Ÿåº¦æå‡: {speedup:.2f}x")
        
        # æ¸…ç†æµ‹è¯•æ•°æ®
        print("ğŸ§¹ æ¸…ç†æµ‹è¯•æ•°æ®...")
        redis_connector.redis_client.delete(*redis_connector.redis_client.keys("test_performance::*"))
    
    # è¾“å‡ºæ€»ç»“
    print(f"\nğŸ“‹ æ€§èƒ½æµ‹è¯•æ€»ç»“")
    print("=" * 50)
    print(f"{'å˜é‡æ•°é‡':<8} {'å•æ¬¡æŸ¥è¯¢(ç§’)':<12} {'æ‰¹é‡æŸ¥è¯¢(ç§’)':<12} {'æ€§èƒ½æå‡(%)':<12} {'é€Ÿåº¦æå‡(x)':<12}")
    print("-" * 70)
    
    for result in results:
        print(f"{result['count']:<8} {result['single_time']:<12.4f} {result['batch_time']:<12.4f} {result['improvement']:<12.2f} {result['speedup']:<12.2f}")
    
    # è®¡ç®—æ€»ä½“å¹³å‡
    if results:
        avg_improvement = sum(r['improvement'] for r in results) / len(results)
        avg_speedup = sum(r['speedup'] for r in results) / len(results)
        print("-" * 70)
        print(f"{'å¹³å‡':<8} {'':<12} {'':<12} {avg_improvement:<12.2f} {avg_speedup:<12.2f}")
        print(f"\nğŸ‰ æ‰¹é‡æŸ¥è¯¢å¹³å‡æ€§èƒ½æå‡: {avg_improvement:.2f}%")
        print(f"ğŸ‰ æ‰¹é‡æŸ¥è¯¢å¹³å‡é€Ÿåº¦æå‡: {avg_speedup:.2f}x")


if __name__ == "__main__":
    main() 