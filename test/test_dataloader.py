#!/usr/bin/env python3
"""
DataLoader å•å…ƒæµ‹è¯•
"""

import pytest
import pandas as pd
from datetime import datetime, timedelta
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from data.redis_connector import RedisConnector
from data.dataloader import DataLoader

JOB_DATETIME = "2025-07-01 10:00:00"


class TestDataLoader:
    """DataLoaderæµ‹è¯•ç±»"""
    
    @pytest.fixture
    def connector(self):
        """åˆ›å»ºRedisè¿æ¥å™¨"""
        connector = RedisConnector()
        connector.clear_all()  # æ¸…ç©ºæµ‹è¯•æ•°æ®
        return connector
    
    @pytest.fixture
    def loader(self, connector):
        """åˆ›å»ºDataLoaderå®ä¾‹"""
        return DataLoader(connector)
    
    def setup_demo_data(self, connector: RedisConnector):
        """è®¾ç½®æ¼”ç¤ºæ•°æ®"""
        print("ğŸ”§ è®¾ç½®æ¼”ç¤ºæ•°æ®...")
        
        # æ¸…ç©ºç°æœ‰æ•°æ®
        connector.clear_all()
        
        # 1. è®¾ç½®ç®€å•å˜é‡æ•°æ®
        connector.store_direct_variable("sams_demand_forecast", "cloud_nbr=1001::city_cn", "æ·±åœ³")
        
        # 2. è®¾ç½®JSONå˜é‡æ•°æ®
        store_info = {
            "mother_store_nbr": "1001",
            "city_en": "Shenzhen",
            "city_cn": "æ·±åœ³"
        }
        connector.store_json_variable("sams_demand_forecast", "cloud_nbr=1001::dim_store", store_info)
        
        # 3. è®¾ç½®æ—¶é—´åºåˆ—æ•°æ®
        base_time = datetime(2024, 1, 1)
        for i in range(10):
            current_date = base_time + timedelta(days=i)
            ds = current_date.strftime("%Y-%m-%d")
            etl_time = (current_date + timedelta(hours=1)).strftime("%Y-%m-%d %H:%M:%S")
            
            data = {
                "ds": ds,
                "etl_time": etl_time,
                "order_quantity": 100 + i * 10,
                "is_activate": 1 if i % 2 == 0 else 0
            }
            
            connector.add_timeseries_point(
                "sams_demand_forecast",
                "cloud_nbr=1001::item_nbr=2001::order_quantity",
                current_date.timestamp(),
                data
            )
        
        # 4. è®¾ç½®å¦ä¸€ä¸ªæ—¶é—´åºåˆ—æ•°æ®
        for i in range(10):
            current_date = base_time + timedelta(days=i)
            ds = current_date.strftime("%Y-%m-%d")
            etl_time = (current_date + timedelta(hours=1)).strftime("%Y-%m-%d %H:%M:%S")
            
            data = {
                "ds": ds,
                "etl_time": etl_time,
                "id1": 50.5 + i * 5.2,
                "id2": 100.5 + i * 5.2,
                "id3": 150.5 + i * 5.2,
            }
            
            connector.add_timeseries_point(
                "sams_demand_forecast",
                "cloud_nbr=1001::job_date=2024-01-01::same_city_forecast",
                current_date.timestamp(),
                data
            )
        
        # 5. æ·»åŠ ä¸€äº›é‡å¤æ•°æ®æ¥æµ‹è¯•å»é‡åŠŸèƒ½
        duplicate_date = base_time + timedelta(days=2)
        duplicate_ds = duplicate_date.strftime("%Y-%m-%d")
        old_etl_time = (duplicate_date + timedelta(minutes=30)).strftime("%Y-%m-%d %H:%M:%S")
        new_etl_time = (duplicate_date + timedelta(hours=2)).strftime("%Y-%m-%d %H:%M:%S")
        
        # æ·»åŠ è¾ƒæ—©çš„æ•°æ®
        old_data = {
            "ds": duplicate_ds,
            "etl_time": old_etl_time,
            "order_quantity": 999,
            "is_activate": 0
        }
        connector.add_timeseries_point(
            "sams_demand_forecast",
            "cloud_nbr=1001::item_nbr=2001::order_quantity",
            duplicate_date.timestamp(),
            old_data
        )
        
        # æ·»åŠ è¾ƒæ–°çš„æ•°æ®
        new_data = {
            "ds": duplicate_ds,
            "etl_time": new_etl_time,
            "order_quantity": 120,
            "is_activate": 1
        }
        connector.add_timeseries_point(
            "sams_demand_forecast",
            "cloud_nbr=1001::item_nbr=2001::order_quantity",
            duplicate_date.timestamp(),
            new_data
        )
        
        print("âœ… æ¼”ç¤ºæ•°æ®è®¾ç½®å®Œæˆ")
    
    def get_demo_config(self):
        """è·å–æ¼”ç¤ºé…ç½®"""
        return {
            "namespace": "sams_demand_forecast",
            "placeholder": [
                {"name": "cloud_nbr", "alias": None, "description": "å±±å§†æé€Ÿè¾¾äº‘ä»“çš„ä»“å·"},
                {"name": "item_nbr", "alias": None, "description": "å•†å“å·"},
                {"name": "biz_date", "alias": None, "description": "é¢„æµ‹æ—¥æœŸ"},
            ],
            "variable": [
                {
                    "name": "df_quantity",
                    "alias": None,
                    "description": None,
                    "namespace": None,
                    "redis_config": {
                        "prefix": [
                            {"key": "cloud_nbr", "value": "${cloud_nbr}"},
                            {"key": "item_nbr", "value": "${item_nbr}"}
                        ],
                        "field": "order_quantity",
                        "type": "timeseries",
                        "from_datetime": "${yyyy-MM-dd-60d}",
                        "to_datetime": "${yyyy-MM-dd-1d}",
                        "drop_duplicate": "keep_latest"
                    }
                },
                {
                    "name": "city_cn",
                    "alias": None,
                    "description": None,
                    "namespace": None,
                    "redis_config": {
                        "prefix": [
                            {"key": "cloud_nbr", "value": "${cloud_nbr}"}
                        ],
                        "field": "city_cn",
                        "type": "value"
                    }
                },
                {
                    "name": "same_city_forecast",
                    "alias": None,
                    "description": None,
                    "namespace": None,
                    "redis_config": {
                        "prefix": [
                            {"key": "cloud_nbr", "value": "${cloud_nbr}"},
                            {"key": "job_date", "value": "${yyyy-MM-dd}"}
                        ],
                        "field": "same_city_forecast",
                        "type": "timeseries",
                        "from_datetime": "${yyyy-MM-dd}",
                        "to_datetime": "${yyyy-MM-dd+13d}",
                        "drop_duplicate": "none"
                    }
                }
            ]
        }
    
    def test_placeholder_validation(self, loader):
        """æµ‹è¯•å ä½ç¬¦éªŒè¯"""
        data_config = {
            "namespace": "test",
            "placeholder": [
                {"name": "param1", "alias": None, "description": "å‚æ•°1"},
                {"name": "param2", "alias": None, "description": "å‚æ•°2"}
            ],
            "variable": []
        }
        
        # æµ‹è¯•ç¼ºå°‘å ä½ç¬¦
        with pytest.raises(ValueError, match="ç¼ºå°‘å¿…éœ€çš„å ä½ç¬¦"):
            loader.load_data(data_config, {"param1": "value1"}, job_datetime=JOB_DATETIME)
        
        # æµ‹è¯•å®Œæ•´å ä½ç¬¦
        result = loader.load_data(data_config, {"param1": "value1", "param2": "value2"}, job_datetime=JOB_DATETIME)
        assert result == {}
    
    def test_redis_key_building(self, loader):
        """æµ‹è¯•Redisé”®æ„å»º"""
        prefixes = [
            {"key": "cloud_nbr", "value": "${cloud_nbr}"},
            {"key": "item_nbr", "value": "${item_nbr}"}
        ]
        placeholders = {"cloud_nbr": "1001", "item_nbr": "2001"}
        
        redis_key = loader._build_redis_key("test_ns", "timeseries", prefixes, "test_field", placeholders, JOB_DATETIME)
        expected_key = "test_ns::timeseries::cloud_nbr=1001::item_nbr=2001::test_field"
        assert redis_key == expected_key
    
    def test_placeholder_replacement(self, loader):
        """æµ‹è¯•å ä½ç¬¦æ›¿æ¢"""
        placeholders = {"name": "test", "value": "123"}
        
        # æµ‹è¯•æ­£å¸¸æ›¿æ¢
        result = loader._replace_placeholders("${name}_${value}", placeholders)
        assert result == "test_123"
        
        # æµ‹è¯•ç¼ºå°‘å ä½ç¬¦
        with pytest.raises(ValueError, match="å ä½ç¬¦ 'missing' æœªæ‰¾åˆ°"):
            loader._replace_placeholders("${name}_${missing}", placeholders)
    
    def test_load_simple_variable_value(self, connector, loader):
        """æµ‹è¯•åŠ è½½ç®€å•å˜é‡ï¼ˆvalueç±»å‹ï¼‰"""
        # è®¾ç½®æµ‹è¯•æ•°æ®
        connector.store_direct_variable("test_ns", "cloud_nbr=1001::test_field", "test_value")
        
        # é…ç½®
        redis_config = {
            "prefix": [{"key": "cloud_nbr", "value": "${cloud_nbr}"}],
            "field": "test_field",
            "type": "value"
        }
        
        # åŠ è½½æ•°æ®
        result = loader._load_simple_variable(redis_config, "test_ns", {"cloud_nbr": "1001"}, job_datetime=JOB_DATETIME)
        assert result == "test_value"
    
    def test_load_simple_variable_json(self, connector, loader):
        """æµ‹è¯•åŠ è½½ç®€å•å˜é‡ï¼ˆjsonç±»å‹ï¼‰"""
        # è®¾ç½®æµ‹è¯•æ•°æ®
        test_data = {"name": "test", "value": 123}
        connector.store_json_variable("test_ns", "cloud_nbr=1001::test_field", test_data)
        
        # é…ç½®
        redis_config = {
            "prefix": [{"key": "cloud_nbr", "value": "${cloud_nbr}"}],
            "field": "test_field",
            "type": "json"
        }
        
        # åŠ è½½æ•°æ®
        result = loader._load_simple_variable(redis_config, "test_ns", {"cloud_nbr": "1001"}, job_datetime=JOB_DATETIME)
        assert result == test_data
    
    def test_load_timeseries(self, connector, loader):
        """æµ‹è¯•åŠ è½½æ—¶é—´åºåˆ—"""
        # è®¾ç½®æµ‹è¯•æ•°æ®
        base_time = datetime(2024, 1, 1)
        for i in range(3):
            current_date = base_time + timedelta(days=i)
            ds = current_date.strftime("%Y-%m-%d")
            etl_time = (current_date + timedelta(hours=1)).strftime("%Y-%m-%d %H:%M:%S")
            data = {
                "ds": ds,
                "etl_time": etl_time,
                "quantity": 100 + i * 10,
                "status": i % 2
            }
            connector.add_timeseries_point("test_ns", "cloud_nbr=1001::test_series", current_date.timestamp(), data)
        
        # é…ç½®
        variable_config = {
            "redis_config": {
                "prefix": [{"key": "cloud_nbr", "value": "${cloud_nbr}"}],
                "field": "test_series",
                "type": "timeseries"
            }
        }
        
        # åŠ è½½æ•°æ®
        result = loader._load_timeseries(variable_config, "test_ns", {"cloud_nbr": "1001"}, job_datetime=JOB_DATETIME)
        
        # éªŒè¯ç»“æœ
        assert isinstance(result, pd.DataFrame)
        assert len(result) == 3
        assert "ds" in result.columns
        assert "etl_time" in result.columns
        assert "quantity" in result.columns
        assert "status" in result.columns
        
        # éªŒè¯æ•°æ®å€¼
        assert result.iloc[0]["quantity"] == 100
        assert result.iloc[1]["quantity"] == 110
        assert result.iloc[2]["quantity"] == 120
        
        # éªŒè¯æ—¶é—´å­—æ®µç±»å‹
        assert isinstance(result.iloc[0]["ds"], str)
        assert isinstance(result.iloc[0]["etl_time"], str)
    
    def test_timeseries_deduplication(self, connector, loader):
        """æµ‹è¯•æ—¶é—´åºåˆ—å»é‡åŠŸèƒ½"""
        # è®¾ç½®é‡å¤æ•°æ®
        base_date = datetime(2024, 1, 1)
        ds = base_date.strftime("%Y-%m-%d")
        
        # æ·»åŠ è¾ƒæ—©çš„æ•°æ®
        old_etl_time = (base_date + timedelta(minutes=30)).strftime("%Y-%m-%d %H:%M:%S")
        old_data = {
            "ds": ds,
            "etl_time": old_etl_time,
            "id1": 100
        }
        connector.add_timeseries_point("test_ns", "test_series", base_date.timestamp(), old_data)
        
        # æ·»åŠ è¾ƒæ–°çš„æ•°æ®
        new_etl_time = (base_date + timedelta(hours=1)).strftime("%Y-%m-%d %H:%M:%S")
        new_data = {
            "ds": ds,
            "etl_time": new_etl_time,
            "id1": 200
        }
        connector.add_timeseries_point("test_ns", "test_series", base_date.timestamp(), new_data)
        
        # é…ç½®
        variable_config = {
            "redis_config": {
                "prefix": [],
                "field": "test_series",
                "type": "timeseries",
                "drop_duplicate": "keep_latest"
            }
        }
        
        # åŠ è½½æ•°æ®
        result = loader._load_timeseries(variable_config, "test_ns", {}, job_datetime=JOB_DATETIME)
        
        # éªŒè¯å»é‡æ•ˆæœï¼šåº”è¯¥åªæœ‰ä¸€æ¡è®°å½•ï¼Œä¸”æ˜¯è¾ƒæ–°çš„æ•°æ®
        assert len(result) == 1
        assert result.iloc[0]["id1"] == 200
        assert isinstance(result.iloc[0]["etl_time"], str)
        assert "ds" in result.columns
    
    def test_load_data_integration(self, connector, loader):
        """æµ‹è¯•å®Œæ•´çš„æ•°æ®åŠ è½½æµç¨‹"""
        # è®¾ç½®å„ç§ç±»å‹çš„æµ‹è¯•æ•°æ®
        connector.store_direct_variable("test_ns", "cloud_nbr=1001::city", "æ·±åœ³")
        connector.store_json_variable("test_ns", "cloud_nbr=1001::info", {"name": "test", "id": 123})
        
        base_date = datetime(2024, 1, 1)
        ds = base_date.strftime("%Y-%m-%d")
        etl_time = (base_date + timedelta(hours=1)).strftime("%Y-%m-%d %H:%M:%S")
        connector.add_timeseries_point("test_ns", "cloud_nbr=1001::series", base_date.timestamp(), {
            "ds": ds,
            "etl_time": etl_time,
            "id1": 100.5
        })
        
        # é…ç½®
        data_config = {
            "namespace": "test_ns",
            "placeholder": [
                {"name": "cloud_nbr", "alias": None, "description": "äº‘ä»“å·"}
            ],
            "variable": [
                {
                    "name": "city",
                    "redis_config": {
                        "prefix": [{"key": "cloud_nbr", "value": "${cloud_nbr}"}],
                        "field": "city",
                        "type": "value"
                    }
                },
                {
                    "name": "info",
                    "redis_config": {
                        "prefix": [{"key": "cloud_nbr", "value": "${cloud_nbr}"}],
                        "field": "info",
                        "type": "json"
                    }
                },
                {
                    "name": "series",
                    "redis_config": {
                        "prefix": [{"key": "cloud_nbr", "value": "${cloud_nbr}"}],
                        "field": "series",
                        "type": "timeseries"
                    }
                }
            ]
        }
        
        # åŠ è½½æ•°æ®
        result = loader.load_data(data_config, {"cloud_nbr": "1001"}, job_datetime=JOB_DATETIME)
        
        # éªŒè¯ç»“æœ
        assert "city" in result
        assert "info" in result
        assert "series" in result
        
        assert result["city"] == "æ·±åœ³"
        assert result["info"] == {"name": "test", "id": 123}
        assert isinstance(result["series"], pd.DataFrame)
        assert len(result["series"]) == 1
        assert result["series"].iloc[0]["id1"] == 100.5
        
        # éªŒè¯æ—¶é—´åºåˆ—å­—æ®µ
        assert "ds" in result["series"].columns
        assert "etl_time" in result["series"].columns
    
    def test_timeseries_ds_etl_time_fields(self, connector, loader):
        """æµ‹è¯•æ—¶é—´åºåˆ—dså’Œetl_timeå­—æ®µ"""
        # è®¾ç½®æµ‹è¯•æ•°æ®
        base_date = datetime(2024, 1, 1, 12, 30, 45)
        ds = base_date.strftime("%Y-%m-%d")
        etl_time = (base_date + timedelta(hours=1)).strftime("%Y-%m-%d %H:%M:%S")
        connector.add_timeseries_point("test_ns", "test_series", base_date.timestamp(), {
            "ds": ds,
            "etl_time": etl_time,
            "id1": 100.5
        })
        
        # æµ‹è¯•é…ç½®
        variable_config = {
            "redis_config": {
                "prefix": [],
                "field": "test_series",
                "type": "timeseries"
            }
        }
        
        result = loader._load_timeseries(variable_config, "test_ns", {}, job_datetime=JOB_DATETIME)
        
        # éªŒè¯dså’Œetl_timeå­—æ®µ
        assert len(result) == 1
        assert result.iloc[0]["ds"] == ds
        assert result.iloc[0]["etl_time"] == etl_time
        assert result.iloc[0]["id1"] == 100.5
    
    def test_drop_duplicate_strategies(self, connector, loader):
        """æµ‹è¯•ä¸åŒçš„å»é‡ç­–ç•¥"""
        # è®¾ç½®é‡å¤æ•°æ®
        base_date = datetime(2024, 1, 1)
        timestamp = base_date.timestamp()
        
        # æ·»åŠ è¾ƒæ—©çš„æ•°æ®
        old_etl_time = (base_date + timedelta(minutes=30)).strftime("%Y-%m-%d %H:%M:%S")
        old_data = {
            "ds": "2024-01-01",
            "etl_time": old_etl_time,
            "value": 100
        }
        connector.add_timeseries_point("test_ns", "test_series", timestamp, old_data)
        
        # æ·»åŠ è¾ƒæ–°çš„æ•°æ®
        new_etl_time = (base_date + timedelta(hours=1)).strftime("%Y-%m-%d %H:%M:%S")
        new_data = {
            "ds": "2024-01-01",
            "etl_time": new_etl_time,
            "value": 200
        }
        connector.add_timeseries_point("test_ns", "test_series", timestamp, new_data)
        
        # æµ‹è¯• "none" ç­–ç•¥ - åº”è¯¥ä¿ç•™æ‰€æœ‰æ•°æ®
        variable_config_none = {
            "redis_config": {
                "prefix": [],
                "field": "test_series",
                "type": "timeseries",
                "drop_duplicate": "none"
            }
        }
        result_none = loader._load_timeseries(variable_config_none, "test_ns", {}, job_datetime=JOB_DATETIME)
        assert len(result_none) == 2  # ä¿ç•™æ‰€æœ‰æ•°æ®
        
        # æµ‹è¯• "keep_latest" ç­–ç•¥ - åº”è¯¥åªä¿ç•™æœ€æ–°çš„æ•°æ®
        variable_config_latest = {
            "redis_config": {
                "prefix": [],
                "field": "test_series",
                "type": "timeseries",
                "drop_duplicate": "keep_latest"
            }
        }
        result_latest = loader._load_timeseries(variable_config_latest, "test_ns", {}, job_datetime=JOB_DATETIME)
        assert len(result_latest) == 1  # åªä¿ç•™ä¸€æ¡è®°å½•
        assert result_latest.iloc[0]["value"] == 200  # ä¿ç•™æœ€æ–°çš„æ•°æ®
        assert result_latest.iloc[0]["etl_time"] == new_etl_time  # ä¿ç•™æœ€æ–°çš„etl_time
    
    def test_demo_integration(self, connector, loader):
        """æµ‹è¯•å®Œæ•´çš„demoé›†æˆåœºæ™¯"""
        # è®¾ç½®æ¼”ç¤ºæ•°æ®
        self.setup_demo_data(connector)
        
        # è·å–é…ç½®
        data_config = self.get_demo_config()
        
        # ç¤ºä¾‹å ä½ç¬¦å€¼
        placeholders = {
            "cloud_nbr": "1001",
            "item_nbr": "2001",
            "biz_date": "2024-01-15"
        }
        
        # ä½œä¸šæ—¥æœŸæ—¶é—´
        job_datetime = "2024-01-15 10:30:00"
        
        # åŠ è½½æ•°æ®
        data = loader.load_data(data_config, placeholders, job_datetime)

        print("data: ", data)
        print("job_datetime: ", job_datetime)
        print("placeholders: ", placeholders)
        print("data['df_quantity']: ", data['df_quantity'])
        print("data['city_cn']: ", data['city_cn'])
        print("data['same_city_forecast']: ", data['same_city_forecast'])
        
        # éªŒè¯ç»“æœ
        assert len(data) == 3
        assert "df_quantity" in data
        assert "city_cn" in data
        assert "same_city_forecast" in data
        
        # éªŒè¯ç®€å•å˜é‡
        assert data["city_cn"] == "æ·±åœ³"
        
        # éªŒè¯DataFrame
        assert isinstance(data["df_quantity"], pd.DataFrame)
        assert isinstance(data["same_city_forecast"], pd.DataFrame)
        
        # éªŒè¯DataFrameåˆ—ç»“æ„
        if len(data["df_quantity"]) > 0:
            expected_columns = ["ds", "etl_time", "order_quantity", "is_activate"]
            assert all(col in data["df_quantity"].columns for col in expected_columns)
        
        if len(data["same_city_forecast"]) > 0:
            expected_columns = ["ds", "etl_time", "id1", "id2", "id3"]
            assert all(col in data["same_city_forecast"].columns for col in expected_columns)


if __name__ == "__main__":
    print("ğŸ§ª è¿è¡ŒDataLoaderæµ‹è¯•")
    pytest.main([__file__, "-v"]) 