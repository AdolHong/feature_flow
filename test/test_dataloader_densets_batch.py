 #!/usr/bin/env python3
"""
æµ‹è¯• DataLoader ä¸­ densets çš„æ‰¹é‡åŠ è½½åŠŸèƒ½
"""

import pytest
import pandas as pd
from datetime import datetime, timedelta
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from data.redis_connector import RedisConnector
from data.dataloader import DataLoader

JOB_DATETIME = "2025-07-01 10:00:00"


class TestDataLoaderDensetsBatch:
    """DataLoader densets æ‰¹é‡åŠ è½½æµ‹è¯•ç±»"""
    
    @pytest.fixture
    def connector(self):
        """åˆ›å»ºRedisè¿æ¥å™¨"""
        connector = RedisConnector()
        connector.clear_all()  # æ¸…ç©ºæµ‹è¯•æ•°æ®
        return connector
    
    @pytest.fixture
    def loader(self, connector):
        """åˆ›å»ºDataLoaderå®ä¾‹"""
        return DataLoader(connector)
    
    def setup_densets_data(self, connector: RedisConnector):
        """è®¾ç½® densets æ¼”ç¤ºæ•°æ®"""
        print("ğŸ”§ è®¾ç½® densets æ¼”ç¤ºæ•°æ®...")
        
        # æ¸…ç©ºç°æœ‰æ•°æ®
        connector.clear_all()
        
        # 1. è®¾ç½®ç®€å•å˜é‡æ•°æ®
        connector.store_direct_variable("test_densets", "cloud_nbr=1001::city_cn", "æ·±åœ³")
        
        # 2. è®¾ç½®JSONå˜é‡æ•°æ®
        store_info = {
            "mother_store_nbr": "1001",
            "city_en": "Shenzhen",
            "city_cn": "æ·±åœ³"
        }
        connector.store_json_variable("test_densets", "cloud_nbr=1001::dim_store", store_info)
        
        # 3. è®¾ç½® densets æ•°æ®
        base_time = datetime(2024, 1, 1)
        for i in range(5):
            current_date = base_time + timedelta(days=i)
            timestamp = current_date.timestamp()
            
            # ä½¿ç”¨ schema æ ¼å¼çš„æ•°æ®
            row_data = f"2024-01-{i+1:02d},2024-01-{i+1:02d},{100 + i * 10:.1f},2024-01-{i+1:02d} 10:00:00"
            
            connector.add_densets_point(
                "test_densets",
                "cloud_nbr=1001::item_nbr=2001::fcst_data",
                timestamp,
                row_data
            )
        
        # 4. è®¾ç½®å¦ä¸€ä¸ª densets æ•°æ®ï¼ˆä¸å¸¦ schemaï¼‰
        for i in range(3):
            current_date = base_time + timedelta(days=i)
            timestamp = current_date.timestamp()
            
            # ç®€å•çš„é€—å·åˆ†éš”æ•°æ®
            row_data = f"value1_{i},value2_{i},{50.5 + i * 5.2}"
            
            connector.add_densets_point(
                "test_densets",
                "cloud_nbr=1001::simple_data",
                timestamp,
                row_data
            )
        
        print("âœ… densets æ¼”ç¤ºæ•°æ®è®¾ç½®å®Œæˆ")
    
    def get_densets_config(self):
        """è·å– densets æ¼”ç¤ºé…ç½®"""
        return {
            "namespace": "test_densets",
            "placeholder": [
                {"name": "cloud_nbr", "alias": None, "description": "äº‘ä»“å·"},
                {"name": "item_nbr", "alias": None, "description": "å•†å“å·"},
            ],
            "variable": [
                {
                    "name": "city_cn",
                    "alias": None,
                    "description": None,
                    "namespace": None,
                    "redis_config": {
                        "prefix": [
                            {"key": "cloud_nbr", "value": "${cloud_nbr}"}
                        ],
                        "field": "city_cn",
                        "type": "value"
                    }
                },
                {
                    "name": "dim_store",
                    "alias": None,
                    "description": None,
                    "namespace": None,
                    "redis_config": {
                        "prefix": [
                            {"key": "cloud_nbr", "value": "${cloud_nbr}"}
                        ],
                        "field": "dim_store",
                        "type": "json"
                    }
                },
                {
                    "name": "fcst_data",
                    "alias": None,
                    "description": None,
                    "namespace": None,
                    "redis_config": {
                        "prefix": [
                            {"key": "cloud_nbr", "value": "${cloud_nbr}"},
                            {"key": "item_nbr", "value": "${item_nbr}"}
                        ],
                        "field": "fcst_data",
                        "type": "densets",
                        "from_datetime": "${yyyy-MM-dd-5d}",
                        "to_datetime": "${yyyy-MM-dd+5d}",
                        "drop_duplicate": "none",
                        "split": ",",
                        "schema": "job_date:string,fcst_date:string,fcst_qty:double,etl_time:string"
                    }
                },
                {
                    "name": "simple_data",
                    "alias": None,
                    "description": None,
                    "namespace": None,
                    "redis_config": {
                        "prefix": [
                            {"key": "cloud_nbr", "value": "${cloud_nbr}"}
                        ],
                        "field": "simple_data",
                        "type": "densets",
                        "from_datetime": "${yyyy-MM-dd-3d}",
                        "to_datetime": "${yyyy-MM-dd+3d}",
                        "drop_duplicate": "none",
                        "split": ","
                    }
                }
            ]
        }
    
    def test_densets_batch_loading(self, connector, loader):
        """æµ‹è¯• densets æ‰¹é‡åŠ è½½åŠŸèƒ½"""
        # è®¾ç½®æ¼”ç¤ºæ•°æ®
        self.setup_densets_data(connector)
        
        # è·å–é…ç½®
        data_config = self.get_densets_config()
        
        # ç¤ºä¾‹å ä½ç¬¦å€¼
        placeholders = {
            "cloud_nbr": "1001",
            "item_nbr": "2001"
        }
        
        # ä½œä¸šæ—¥æœŸæ—¶é—´
        job_datetime = "2024-01-15 10:30:00"
        
        # æµ‹è¯•å•æ¬¡åŠ è½½
        print("\nğŸ”„ æµ‹è¯•å•æ¬¡åŠ è½½...")
        single_result = loader.load_data(data_config, placeholders, job_datetime)
        
        # æµ‹è¯•æ‰¹é‡åŠ è½½
        print("\nğŸ”„ æµ‹è¯•æ‰¹é‡åŠ è½½...")
        batch_result = loader.load_data_batch(data_config, placeholders, job_datetime)
        
        # éªŒè¯ç»“æœä¸€è‡´æ€§
        assert len(single_result) == len(batch_result)
        assert set(single_result.keys()) == set(batch_result.keys())
        
        # éªŒè¯ç®€å•å˜é‡
        assert single_result["city_cn"] == batch_result["city_cn"] == "æ·±åœ³"
        
        # éªŒè¯ JSON å˜é‡
        expected_store_info = {
            "mother_store_nbr": "1001",
            "city_en": "Shenzhen",
            "city_cn": "æ·±åœ³"
        }
        assert single_result["dim_store"] == batch_result["dim_store"] == expected_store_info
        
        # éªŒè¯ densets æ•°æ®
        assert isinstance(single_result["fcst_data"], pd.DataFrame)
        assert isinstance(batch_result["fcst_data"], pd.DataFrame)
        assert len(single_result["fcst_data"]) == len(batch_result["fcst_data"])
        
        # éªŒè¯å¸¦ schema çš„ densets æ•°æ®åˆ—
        if len(single_result["fcst_data"]) > 0:
            expected_columns = ["job_date", "fcst_date", "fcst_qty", "etl_time"]
            assert all(col in single_result["fcst_data"].columns for col in expected_columns)
            assert all(col in batch_result["fcst_data"].columns for col in expected_columns)
            
            # éªŒè¯æ•°æ®ç±»å‹
            assert single_result["fcst_data"]["fcst_qty"].dtype in ['float64', 'float32']
            assert batch_result["fcst_data"]["fcst_qty"].dtype in ['float64', 'float32']
        
        # éªŒè¯ä¸å¸¦ schema çš„ densets æ•°æ®
        assert isinstance(single_result["simple_data"], pd.DataFrame)
        assert isinstance(batch_result["simple_data"], pd.DataFrame)
        assert len(single_result["simple_data"]) == len(batch_result["simple_data"])
        
        if len(single_result["simple_data"]) > 0:
            # ä¸å¸¦ schema æ—¶åº”è¯¥æœ‰ __values__ åˆ—
            assert "__values__" in single_result["simple_data"].columns
            assert "__values__" in batch_result["simple_data"].columns
        
        print("âœ… densets æ‰¹é‡åŠ è½½æµ‹è¯•é€šè¿‡")
    
    def test_densets_with_timeseries_mixed(self, connector, loader):
        """æµ‹è¯• densets å’Œæ—¶é—´åºåˆ—æ··åˆçš„æ‰¹é‡åŠ è½½"""
        # è®¾ç½®æ··åˆæ•°æ®
        connector.clear_all()
        
        # è®¾ç½®ç®€å•å˜é‡
        connector.store_direct_variable("test_mixed", "cloud_nbr=1001::city", "åŒ—äº¬")
        
        # è®¾ç½® densets æ•°æ®
        base_time = datetime(2024, 1, 1)
        for i in range(3):
            current_date = base_time + timedelta(days=i)
            timestamp = current_date.timestamp()
            row_data = f"2024-01-{i+1:02d},{100 + i * 10:.1f}"
            connector.add_densets_point("test_mixed", "cloud_nbr=1001::densets_data", timestamp, row_data)
        
        # è®¾ç½®æ—¶é—´åºåˆ—æ•°æ®
        for i in range(3):
            current_date = base_time + timedelta(days=i)
            ds = current_date.strftime("%Y-%m-%d")
            etl_time = (current_date + timedelta(hours=1)).strftime("%Y-%m-%d %H:%M:%S")
            data = {
                "ds": ds,
                "etl_time": etl_time,
                "quantity": 200 + i * 20
            }
            connector.add_timeseries_point("test_mixed", "cloud_nbr=1001::timeseries_data", current_date.timestamp(), data)
        
        # é…ç½®
        data_config = {
            "namespace": "test_mixed",
            "placeholder": [
                {"name": "cloud_nbr", "alias": None, "description": "äº‘ä»“å·"}
            ],
            "variable": [
                {
                    "name": "city",
                    "redis_config": {
                        "prefix": [{"key": "cloud_nbr", "value": "${cloud_nbr}"}],
                        "field": "city",
                        "type": "value"
                    }
                },
                {
                    "name": "densets_data",
                    "redis_config": {
                        "prefix": [{"key": "cloud_nbr", "value": "${cloud_nbr}"}],
                        "field": "densets_data",
                        "type": "densets",
                        "split": ",",
                        "schema": "date:string,value:double"
                    }
                },
                {
                    "name": "timeseries_data",
                    "redis_config": {
                        "prefix": [{"key": "cloud_nbr", "value": "${cloud_nbr}"}],
                        "field": "timeseries_data",
                        "type": "timeseries"
                    }
                }
            ]
        }
        
        placeholders = {"cloud_nbr": "1001"}
        job_datetime = "2024-01-15 10:30:00"
        
        # æµ‹è¯•æ‰¹é‡åŠ è½½
        batch_result = loader.load_data_batch(data_config, placeholders, job_datetime)
        
        # éªŒè¯ç»“æœ
        assert len(batch_result) == 3
        assert "city" in batch_result
        assert "densets_data" in batch_result
        assert "timeseries_data" in batch_result
        
        # éªŒè¯ç®€å•å˜é‡
        assert batch_result["city"] == "åŒ—äº¬"
        
        # éªŒè¯ densets æ•°æ®
        assert isinstance(batch_result["densets_data"], pd.DataFrame)
        assert len(batch_result["densets_data"]) == 3
        assert "date" in batch_result["densets_data"].columns
        assert "value" in batch_result["densets_data"].columns
        
        # éªŒè¯æ—¶é—´åºåˆ—æ•°æ®
        assert isinstance(batch_result["timeseries_data"], pd.DataFrame)
        assert len(batch_result["timeseries_data"]) == 3
        assert "ds" in batch_result["timeseries_data"].columns
        assert "etl_time" in batch_result["timeseries_data"].columns
        assert "quantity" in batch_result["timeseries_data"].columns
        
        print("âœ… æ··åˆæ•°æ®ç±»å‹æ‰¹é‡åŠ è½½æµ‹è¯•é€šè¿‡")
    
    def test_densets_deduplication_in_batch(self, connector, loader):
        """æµ‹è¯• densets åœ¨æ‰¹é‡åŠ è½½ä¸­çš„å»é‡åŠŸèƒ½"""
        # è®¾ç½®é‡å¤æ•°æ®
        connector.clear_all()
        
        base_time = datetime(2024, 1, 1)
        timestamp = base_time.timestamp()
        
        # æ·»åŠ é‡å¤çš„ densets æ•°æ®
        row_data1 = "2024-01-01,100.5,2024-01-01 09:00:00"
        row_data2 = "2024-01-01,200.5,2024-01-01 10:00:00"  # æ›´æ–°çš„æ•°æ®
        
        connector.add_densets_point("test_dedup", "test_series", timestamp, row_data1)
        connector.add_densets_point("test_dedup", "test_series", timestamp, row_data2)
        
        # é…ç½®
        data_config = {
            "namespace": "test_dedup",
            "placeholder": [],
            "variable": [
                {
                    "name": "test_data",
                    "redis_config": {
                        "prefix": [],
                        "field": "test_series",
                        "type": "densets",
                        "split": ",",
                        "schema": "date:string,value:double,etl_time:string",
                        "drop_duplicate": "keep_latest"
                    }
                }
            ]
        }
        
        placeholders = {}
        job_datetime = "2024-01-15 10:30:00"
        
        # æµ‹è¯•æ‰¹é‡åŠ è½½
        batch_result = loader.load_data_batch(data_config, placeholders, job_datetime)
        
        # éªŒè¯å»é‡æ•ˆæœ
        assert len(batch_result["test_data"]) == 1
        # åœ¨ densets ä¸­ï¼Œåæ·»åŠ çš„æ•°æ®ä¼šè¦†ç›–å…ˆæ·»åŠ çš„æ•°æ®ï¼Œæ‰€ä»¥åº”è¯¥ä¿ç•™åæ·»åŠ çš„æ•°æ®
        assert batch_result["test_data"].iloc[0]["value"] == 200.5
        assert batch_result["test_data"].iloc[0]["etl_time"] == "2024-01-01 10:00:00"
        
        print("âœ… densets å»é‡åŠŸèƒ½æµ‹è¯•é€šè¿‡")

    def test_densets_load_and_batch_consistency(self, connector, loader):
        """æµ‹è¯• densets ç”¨ load_data å’Œ load_data_batch è¯»å–ä¸€è‡´æ€§"""
        connector.clear_all()
        namespace = "test_consistency"
        series_key = "cloud_nbr=1001::item_nbr=2001::fcst_data"
        schema = "job_date:string,fcst_date:string,fcst_qty:double,etl_time:string"
        base_time = datetime(2024, 1, 1)
        # æ„é€ 3æ¡æ•°æ®
        for i in range(3):
            current_date = base_time + timedelta(days=i)
            timestamp = current_date.timestamp()
            row_data = f"2024-01-{i+1:02d},2024-01-{i+1:02d},{100 + i * 10:.1f},2024-01-{i+1:02d} 10:00:00"
            connector.add_densets_point(namespace, series_key, timestamp, row_data)
        # é…ç½®
        data_config = {
            "namespace": namespace,
            "placeholder": [
                {"name": "cloud_nbr", "alias": None, "description": "äº‘ä»“å·"},
                {"name": "item_nbr", "alias": None, "description": "å•†å“å·"},
            ],
            "variable": [
                {
                    "name": "fcst_data",
                    "redis_config": {
                        "prefix": [
                            {"key": "cloud_nbr", "value": "${cloud_nbr}"},
                            {"key": "item_nbr", "value": "${item_nbr}"}
                        ],
                        "field": "fcst_data",
                        "type": "densets",
                        "from_datetime": "${yyyy-MM-dd-5d}",
                        "to_datetime": "${yyyy-MM-dd+5d}",
                        "drop_duplicate": "none",
                        "split": ",",
                        "schema": schema
                    }
                }
            ]
        }
        placeholders = {"cloud_nbr": "1001", "item_nbr": "2001"}
        job_datetime = "2024-01-15 10:30:00"
        # åˆ†åˆ«è¯»å–
        result1 = loader.load_data(data_config, placeholders, job_datetime)
        result2 = loader.load_data_batch(data_config, placeholders, job_datetime)
        # æ£€æŸ¥key
        assert "fcst_data" in result1 and "fcst_data" in result2
        # æ£€æŸ¥ç±»å‹
        assert isinstance(result1["fcst_data"], pd.DataFrame)
        assert isinstance(result2["fcst_data"], pd.DataFrame)
        # æ£€æŸ¥å†…å®¹å®Œå…¨ä¸€è‡´
        pd.testing.assert_frame_equal(result1["fcst_data"], result2["fcst_data"])
        print("âœ… load_data å’Œ load_data_batch densets è¯»å–ä¸€è‡´æ€§æµ‹è¯•é€šè¿‡")


if __name__ == "__main__":
    print("ğŸ§ª è¿è¡Œ DataLoader densets æ‰¹é‡åŠ è½½æµ‹è¯•")
    pytest.main([__file__, "-v"])